# -*- coding: utf-8 -*-
"""Autograd.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-2ig_yQkXZEbrFNdyiTGmgxiI9LncI3C
"""

import  torch
print(torch.__version__)

"""AutoGrad is the automatic differentiation tool of Pytorch."""

#creating a function that will give us differentaition
def dy_dx(x):
  return 2*x

dy_dx(5)

"""As the complexity of finding the derivative increases it becomes difficult for us to find the derivative.

So autoGrad is tool by using which we can automatically calculate all the derivaties inside a neural network for tensor operations.
"""

#Finding Derivative of a Tensor
x = torch.tensor(3.0,requires_grad=True)
y=x**2

x

#calculating y in the forward direction
y

#calculating y in backward direction, here we aree calculating all the gradients
y.backward()

#here we are checking all the values of the gradients using.grad
x.grad

# y=x**2, z=sin(y), and we need to calculate dz/dx

x= torch.tensor(3.0,requires_grad=True)
y=x**2
z=torch.sin(y)

x

y

z

z.backward()
x.grad

"""Now we will calculate first manually and then using autograd

"""

import torch

# Inputs
x = torch.tensor(6.7)  # Input feature
y = torch.tensor(0.0)  # True label (binary)

w = torch.tensor(1.0)  # Weight
b = torch.tensor(0.0)  # Bias

# Binary Cross-Entropy Loss for scalar
def binary_cross_entropy_loss(prediction, target):
    epsilon = 1e-8  # To prevent log(0)
    prediction = torch.clamp(prediction, epsilon, 1 - epsilon)
    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))

# Forward pass
z = w * x + b  # Weighted sum (linear part)
y_pred = torch.sigmoid(z)  # Predicted probability

# Compute binary cross-entropy loss
loss = binary_cross_entropy_loss(y_pred, y)

loss

# Derivatives:
# 1. dL/d(y_pred): Loss with respect to the prediction (y_pred)
dloss_dy_pred = (y_pred - y)/(y_pred*(1-y_pred))

# 2. dy_pred/dz: Prediction (y_pred) with respect to z (sigmoid derivative)
dy_pred_dz = y_pred * (1 - y_pred)

# 3. dz/dw and dz/db: z with respect to w and b
dz_dw = x  # dz/dw = x
dz_db = 1  # dz/db = 1 (bias contributes directly to z)

dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw
dL_db = dloss_dy_pred * dy_pred_dz * dz_db

print(f"Manual Gradient of loss w.r.t weight (dw): {dL_dw}")
print(f"Manual Gradient of loss w.r.t bias (db): {dL_db}")

"""Now using autograd"""

x = torch.tensor(6.7)
y = torch.tensor(0.0)

w = torch.tensor(1.0, requires_grad=True)
b = torch.tensor(0.0, requires_grad=True)

print(w)
print(b)

"""Forward Propagation"""

z = w*x + b
z

y_pred = torch.sigmoid(z)
y_pred

loss = binary_cross_entropy_loss(y_pred, y)
loss

"""Backward Propagation"""

loss.backward()

print(w.grad)
print(b.grad)

"""Applying autograd on vectors"""

x=torch.tensor([1.0,2.0,3.0],requires_grad=True)
print(x)

y=(x**2).mean()
y

y.backward()

#Here we are getting our gradients
x.grad

"""Clearning gradients"""

#whenever we call the backward pass again and again, our gradients gets accmulated

x= torch.tensor([2.0],requires_grad=True)
print(x)

y=x**2
y

y.backward()
x.grad

#but if we again do this

y=x**2
y
y.backward()
x.grad

"""we are getting .8 as grident but befor it was .4, so gradients are getting accumulated,so to avoid this whenever we do multiple passes we need to clear our gradients."""

#and this is done through
x.grad.zero_()

#now if we run our forward pass we will get correct gradient that is .4
y=x**2
y
y.backward()
x.grad

"""Now how to disable Gradient tracking."""

#we need to disable our gradient tracking when our neural network is trained and we need to do prediction,
 #at that time we don't need backward pass, so at that time we can disable gradient tracking and for this we have,
# option 1 - requires_grad_(False)
# option 2 - detach()
# option 3 - torch.no_grad()

x.requires_grad_(False)
x
#here we can see that requires_grad thing is not coming in output, and due to this we won't be able to call y.backward() it will show an error

y.backward()

# Using Detach
x=torch.tensor(2.0,requires_grad=True)
x

z = x.detach()
z
#Here also gradient tracking is disabled

#most easy is Calling nograd function
x=torch.tensor(2.0,requires_grad=True)
x

with torch.no_grad():
  y=x**2
  y

# In this case also y.backward() will not work.

